{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **[One-shot Learning with Memory-Augmented Neural Networks](https://arxiv.org/pdf/1605.06065)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**One-shot learning** (i.e., learning from a single example) is a **challenge for traditional deep neural networks**, which **require large amounts of data** and **long training times**.\n",
    "\n",
    "When a **new piece of data is introduced**, these networks have to readjust their **parameters in an inefficient way**, also **risking forgetting** the **knowledge acquired previously** (catastrophic interference problem).\n",
    "\n",
    "The authors propose **neural networks** with increased **external memory**, such as **[Neural Turing Machines (NTM)](https://arxiv.org/pdf/1410.5401)**. These architectures **allow new information to be stored and recalled quickly**, avoiding classic **iterative learning**. The paper shows that a **neural network with memory can assimilate data** quickly and make accurate predictions even after a few examples.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **success of modern deep learning** relies on **gradient optimization** (Values that indicate how much the **network's weights** should be updated) of high-capacity models, which work well in **large-scale supervised tasks** such as **image classification, speech recognition, and games**.\n",
    "\n",
    "However, these **models need a lot of data and incremental training**, making them unsuitable for situations where rapid learning from a few examples is required (\"one-shot learning\").\n",
    "\n",
    "**This kind of flexible adaptation is typical of human beings**: with just **one** example we can **infer new meanings or behaviors**. \n",
    "**Conventional neural networks fail in this**, as they have to **readjust the weights for each new piece of data**, with the risk of **catastrophic interference**. For this reason, nonparametric methods are often preferred in low-data contexts.\n",
    "\n",
    "_The problem of **catastrophic interference** refers to the fact that when **conventional neural networks**. learn new data, they can **\"forget\"** what they have previously learned – a problem that **humans generally do not have** in such a pronounced way._\n",
    "\n",
    "A possible solution is **[meta-learning](https://machinelearningmastery.com/meta-learning-in-machine-learning/)**(refers to learning algorithms that learn from other learning algorithms.), i.e. learning on **two levels**:\n",
    "\n",
    "1. within the single task (e.g. classify images),\n",
    "2. slow between different tasks, accumulating general knowledge about the structure of problems.\n",
    "\n",
    "**Neural networks with memory**, such as **LSTMs**, have shown **meta-learning capabilities**, but they are **not suitable for contexts** where a lot of **new information needs to be encoded quickly**.\n",
    "We need a **scalable architecture**, with:\n",
    "- **Stable**, content-addressable memory.\n",
    "- A **number of parameters independent** of the memory size.\n",
    "\n",
    "**Recent models** such as **Neural Turing Machines (NTMs) or Memory Networks** do this. The paper proposes a class of networks called **Memory-Augmented Neural Networks (MANNs)**, which use **external memory**.\n",
    "**MANNs** are capable of:\n",
    "- **meta-learning** in tasks with high short- and long-term memory requirements,\n",
    "- classify new **Omniglot classes with human-like accuracy**, Omniglot is an **encyclopedia of writing systems and languages**.\n",
    "- **Estimate complex function**s from a few examples.\n",
    "\n",
    "Their approach **combines**:\n",
    "- **slow learning** of useful **representations through gradient descent**,\n",
    "- and quick storage of **new information via external memory**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meta-Learning Task Methodology**\n",
    "\n",
    "In **meta-learning**, the goal is not simply to minimize a learning cost \\( L \\) on a single dataset \\( D \\), but to **minimize the expected cost** on a **distribution of datasets** \\( p(D) \\):\n",
    "\n",
    "$$\n",
    "\\theta = \\arg\\min_{\\theta} \\mathbb{E}_{D \\sim p(D)} [L(D; \\theta)]\n",
    "\\tag{1}\n",
    "$$\n",
    "\n",
    "To do so, it is essential to structure the task correctly. The authors use an episodic approach: each **episode (or task)** consists of the presentation of a dataset $( D = \\{(x_t, y_t)\\}_{t=1}^T )$, where:\n",
    "- $( x_t )$ is the input (e.g. an image),\n",
    "- $( y_t )$ is the label (classification) or the real value (regression).\n",
    "\n",
    "However, the label $( y_t )$ is **not shown together with the corresponding input**. Instead, the model receives a time sequence like:\n",
    "\n",
    "$$\n",
    "(x_1, \\varnothing), (x_2, y_1), (x_3, y_2), \\dots, (x_T, y_{T-1})\n",
    "$$\n",
    "\n",
    "That is, at time $( t )$, the model sees $( x_t )$ and the label $( y_{t-1} )$, and must **predict $( y_t )$**.\n",
    "\n",
    "> This means that the network must **memorize the data it saw previously** (for which it just received the label) and use it to build a map between input and label, to be used for the next examples.\n",
    "\n",
    "Additionally:\n",
    "- **Labels are shuffled** in each episode, to prevent the model from simply learning to associate classes with fixed symbols in the weights.\n",
    "- The model must **bind input representations to labels dynamically**, using memory.\n",
    "- At the first appearance of a class, the model can only guess. But in subsequent presentations, if it has memorized correctly, it can achieve **perfect accuracy**.\n",
    "\n",
    "This structure forces the model to:\n",
    "- **meta-learn** to build input-label links regardless of the specific content,\n",
    "- generalize an **association method** that works on any new class or function, learned in a single episode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation of the Input Sequence for Meta-Learning\n",
    "\n",
    "The basic idea of the input sequence used in the paper for meta-learning is as follows:\n",
    "\n",
    "1. **Show the model a sequence of pairs**:\n",
    "   $$\n",
    "   (x_t, y_{t-1})\n",
    "   $$\n",
    "   - Where $(x_t)$ is the current input, and $(y_{t-1})$ is the output from the previous step.\n",
    "\n",
    "2. **Ask the model to predict**:\n",
    "   $$\n",
    "   y_t\n",
    "   $$\n",
    "   - The goal is for the model to learn to predict the current output $(y_t)$ based on the given input-output pair $((x_t, y_{t-1}))$.\n",
    "\n",
    "This approach helps the model learn patterns and dependencies in the sequence, which is a key concept in meta-learning.\n",
    "\n",
    "- **Feature** → the info you give to the model (input)\n",
    "- **Class** → what you want the model to predict (output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequence (x_t, y_{t-1}):\n",
      "\n",
      "t=0 -> x_t: [8 9], y_(t-1): None\n",
      "t=1 -> x_t: [5 6], y_(t-1): C\n",
      "t=2 -> x_t: [5 5], y_(t-1): B\n",
      "t=3 -> x_t: [9 9], y_(t-1): B\n",
      "t=4 -> x_t: [1 1], y_(t-1): C\n",
      "t=5 -> x_t: [1 2], y_(t-1): A\n",
      "\n",
      "Target to predict (y_t):\n",
      "\n",
      "t=0 -> y_t: C\n",
      "t=1 -> y_t: B\n",
      "t=2 -> y_t: B\n",
      "t=3 -> y_t: C\n",
      "t=4 -> y_t: A\n",
      "t=5 -> y_t: A\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Let's assume we have 3 classes: A, B, C\n",
    "# Each input is a vector of 2 numbers, and each class has 2 examples\n",
    "class_names = ['A', 'B', 'C']\n",
    "inputs = np.array([\n",
    "    [1, 1],  # class A\n",
    "    [1, 2],  # class A\n",
    "    [5, 5],  # class B\n",
    "    [5, 6],  # class B\n",
    "    [9, 9],  # class C\n",
    "    [8, 9],  # class C\n",
    "])\n",
    "\n",
    "# Corresponding labels\n",
    "labels = np.array(['A', 'A', 'B', 'B', 'C', 'C'])\n",
    "\n",
    "# Shuffle the dataset and labels\n",
    "perm = np.random.permutation(len(inputs))\n",
    "inputs = inputs[perm]\n",
    "labels = labels[perm]\n",
    "\n",
    "# Now we simulate the temporal sequence (x_t, y_{t-1})\n",
    "# Where the first y is None, as described in the paper\n",
    "print(\"Input sequence (x_t, y_{t-1}):\\n\")\n",
    "for t in range(len(inputs)):\n",
    "    x_t = inputs[t]\n",
    "    y_prev = labels[t-1] if t > 0 else None\n",
    "    print(f\"t={t} -> x_t: {x_t}, y_(t-1): {y_prev}\")\n",
    "\n",
    "# Model task: predict y_t for each x_t\n",
    "print(\"\\nTarget to predict (y_t):\\n\")\n",
    "for t in range(len(inputs)):\n",
    "    print(f\"t={t} -> y_t: {labels[t]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Each **x_t** is a set of **features** → e.g. [5, 6].\n",
    "- Each **y_t** is a **class** → e.g. \"B\".\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "- At first the model **knows nothing**: $( x_0 )$ is presented without a label → it has to **guess**.\n",
    "- Then, at each step, it receives a new $( x_t )$ with the **label of the previous step** $( y_{t-1} )$ → it has to use **internal memory** to associate inputs with labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **What really happens in this meta-learning task?**\n",
    "\n",
    "1. We have an **episode**, i.e. a small learning \"story\", where:\n",
    "- Each input $( x_t )$ is a feature (e.g. image, vector),\n",
    "- Each label $( y_t )$ is a class associated with that input (e.g. \"A\", \"B\", \"C\").\n",
    "\n",
    "2. The **classes change from episode to episode**: the network **cannot learn them in the weights** (they are shuffled on purpose each time).\n",
    "\n",
    "3. The model sees a sequence like this:\n",
    "\n",
    "$$\n",
    "(x_1, \\varnothing), (x_2, y_1), (x_3, y_2), \\dots, (x_T, y_{T-1})\n",
    "$$\n",
    "\n",
    "Where at each step it **receives** the current data $( x_t )$ and the label of the previous $( y_{t-1} )$, but it has to **predict** the current label $( y_t )$.\n",
    "\n",
    "4. The **real test** is: can the network *memorize on the fly* the pairings $( x \\to y )$ and use them to recognize the same class when it sees it again?\n",
    "\n",
    "The network:\n",
    "- At time `t=1`, it receives `x_1` and nothing → it has to **guess**.\n",
    "- At time `t=2`, it receives `x_2` and `y_1=\"B\"` → it has to predict `y_2=\"A\"` → **it can't know, new class, guess again**.\n",
    "But...\n",
    "- When it sees `x_3 = [1,2]` again, if it saw `x_2 = [1,1]` with `y_2=\"A\"`, it can deduce \"this looks like A\" → **correct prediction**.\n",
    "- And so on.\n",
    "\n",
    "All this **without** memorizing A = class 0, B = class 1, etc., because those labels change every episode.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Memory-Augmented Model – Summary**\n",
    "#### **Neural Turing Machines (NTM)**\n",
    "\n",
    "**Neural Turing Machines (NTM)** are a **differentiable realization of MANN** (Memory-Augmented Neural Network). They are composed of:\n",
    "- A **controller** (a feed-forward network or an LSTM),\n",
    "- An **external memory module**,\n",
    "- Several **read/write heads** that interact with the memory.\n",
    "\n",
    "---\n",
    "\n",
    "### **What do NTMs do?**\n",
    "\n",
    "- They allow **writing and reading vectors from external memory** at each time-step.\n",
    "- They are capable of both **short-term memory** (thanks to external memory) and **long-term memory** (via gradient-updated weights).\n",
    "- This makes them **perfect for meta-learning**, where you need to:\n",
    "- Learn **quickly** (just one exposure),\n",
    "- And maintain **general structures** over time.\n",
    "\n",
    "---\n",
    "\n",
    "### **How ​​does memory access work?**\n",
    "\n",
    "When the controller receives an input $( x_t )$, it generates a **key** $( k_t )$, which is used to:\n",
    "- **Write** to memory $( M_t )$,\n",
    "- Or **read** from it.\n",
    "\n",
    "#### **Formula 1 – Cosine Similarity**\n",
    "\n",
    "To read, we compute the **cosine similarity** between the key $( k_t )$ and each row of memory $( M_t(i) )$:\n",
    "\n",
    "$$\n",
    "K(k_t, M_t(i)) = \\frac{k_t \\cdot M_t(i)}{\\|k_t\\| \\cdot \\|M_t(i)\\|}\n",
    "\\tag{2}\n",
    "$$\n",
    "\n",
    "#### **Formula 2 – Read Weights (softmax)**\n",
    "\n",
    "This similarity is then normalized with softmax to obtain the **read weights vector** $( w^r_t(i) )$:\n",
    "\n",
    "$$\n",
    "w^r_t(i) = \\frac{\\exp(K(k_t, M_t(i)))}{\\sum_j \\exp(K(k_t, M_t(j)))}\n",
    "\\tag{3}\n",
    "$$\n",
    "\n",
    "#### **Formula 3 – Read from Memory**\n",
    "\n",
    "The **read** from memory is done by making a **weighted average** of the memory rows with the weights just calculated:\n",
    "\n",
    "$$\n",
    "r_t = \\sum_i w^r_t(i) \\cdot M_t(i)\n",
    "\\tag{4}\n",
    "$$\n",
    "\n",
    "### **How ​​is the memory read?**\n",
    "\n",
    "The read vector $( r_t )$ is:\n",
    "- Sent to the **classifier** (e.g. a softmax layer),\n",
    "- Used as **additional input** for the next state of the controller.\n",
    "\n",
    "This creates a **continuous loop** of write-read-adapt, which allows the network to:\n",
    "- Store new data **instantly**,\n",
    "- Reuse it in real time to make **accurate predictions** even with just one example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ace_tools_open\n",
      "  Using cached ace_tools_open-0.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\dell\\anaconda3\\envs\\ai_env\\lib\\site-packages (from ace_tools_open) (2.2.3)\n",
      "Collecting itables (from ace_tools_open)\n",
      "  Using cached itables-2.2.5-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: IPython in c:\\users\\dell\\anaconda3\\envs\\ai_env\\lib\\site-packages (from ace_tools_open) (8.34.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\dell\\anaconda3\\envs\\ai_env\\lib\\site-packages (from IPython->ace_tools_open) (0.4.6)\n",
      "Requirement already satisfied: decorator in c:\\users\\dell\\anaconda3\\envs\\ai_env\\lib\\site-packages (from IPython->ace_tools_open) (5.2.1)\n",
      "Requirement already satisfied: exceptiongroup in c:\\users\\dell\\anaconda3\\envs\\ai_env\\lib\\site-packages (from IPython->ace_tools_open) (1.2.2)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\dell\\anaconda3\\envs\\ai_env\\lib\\site-packages (from IPython->ace_tools_open) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\dell\\anaconda3\\envs\\ai_env\\lib\\site-packages (from IPython->ace_tools_open) (0.1.7)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in c:\\users\\dell\\anaconda3\\envs\\ai_env\\lib\\site-packages (from IPython->ace_tools_open) (3.0.50)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\dell\\anaconda3\\envs\\ai_env\\lib\\site-packages (from IPython->ace_tools_open) (2.19.1)\n",
      "Requirement already satisfied: stack_data in c:\\users\\dell\\anaconda3\\envs\\ai_env\\lib\\site-packages (from IPython->ace_tools_open) (0.6.3)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in c:\\users\\dell\\anaconda3\\envs\\ai_env\\lib\\site-packages (from IPython->ace_tools_open) (5.14.3)\n",
      "Requirement already satisfied: typing_extensions>=4.6 in c:\\users\\dell\\anaconda3\\envs\\ai_env\\lib\\site-packages (from IPython->ace_tools_open) (4.12.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\dell\\anaconda3\\envs\\ai_env\\lib\\site-packages (from itables->ace_tools_open) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\dell\\anaconda3\\envs\\ai_env\\lib\\site-packages (from pandas->ace_tools_open) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\dell\\anaconda3\\envs\\ai_env\\lib\\site-packages (from pandas->ace_tools_open) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\dell\\anaconda3\\envs\\ai_env\\lib\\site-packages (from pandas->ace_tools_open) (2025.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\users\\dell\\anaconda3\\envs\\ai_env\\lib\\site-packages (from jedi>=0.16->IPython->ace_tools_open) (0.8.4)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\dell\\anaconda3\\envs\\ai_env\\lib\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->IPython->ace_tools_open) (0.2.13)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\dell\\anaconda3\\envs\\ai_env\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->ace_tools_open) (1.17.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\dell\\anaconda3\\envs\\ai_env\\lib\\site-packages (from stack_data->IPython->ace_tools_open) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\dell\\anaconda3\\envs\\ai_env\\lib\\site-packages (from stack_data->IPython->ace_tools_open) (3.0.0)\n",
      "Requirement already satisfied: pure_eval in c:\\users\\dell\\anaconda3\\envs\\ai_env\\lib\\site-packages (from stack_data->IPython->ace_tools_open) (0.2.3)\n",
      "Using cached ace_tools_open-0.1.0-py3-none-any.whl (3.0 kB)\n",
      "Using cached itables-2.2.5-py3-none-any.whl (1.4 MB)\n",
      "Installing collected packages: itables, ace_tools_open\n",
      "Successfully installed ace_tools_open-0.1.0 itables-2.2.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install ace_tools_open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulated NTM Memory (Memory Slots and Read Weights):\n",
      "\n",
      "   dim1  dim2  dim3  read_weight\n",
      "0   1.0   0.0   0.0     0.404610\n",
      "1   0.0   1.0   0.0     0.148848\n",
      "2   0.0   0.0   0.0     0.148848\n",
      "3   0.0   0.0   0.0     0.148848\n",
      "4   0.0   0.0   0.0     0.148848\n",
      "\n",
      "Read Result (r_t):\n",
      " [0.40460967 0.14884758 0.        ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ================================\n",
    "# Minimalistic NTM-like simulation\n",
    "# ================================\n",
    "\n",
    "# Parameters\n",
    "dim_feature = 3  # Size of x_t vectors\n",
    "mem_slots = 5    # Number of memory rows\n",
    "mem_dim = dim_feature  # Size of memory rows (same as x_t)\n",
    "\n",
    "# Initialize memory to zeros\n",
    "M = np.zeros((mem_slots, mem_dim))\n",
    "\n",
    "# Cosine similarity function\n",
    "def cosine_similarity(k, m):\n",
    "    norm_k = np.linalg.norm(k)\n",
    "    norm_m = np.linalg.norm(m, axis=1)\n",
    "    dot = m @ k\n",
    "    return dot / (norm_k * norm_m + 1e-8)  # Avoid division by zero\n",
    "\n",
    "# Softmax function\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "# Input simulation (3 examples)\n",
    "x_samples = np.array([\n",
    "    [1.0, 0.0, 0.0],  # Class A\n",
    "    [0.0, 1.0, 0.0],  # Class B\n",
    "    [1.0, 0.0, 0.0],  # Same class A (expected recognition)\n",
    "])\n",
    "\n",
    "# Step 1: Write the first two examples into memory\n",
    "M[0] = x_samples[0]  # Write the first vector\n",
    "M[1] = x_samples[1]  # Write the second vector\n",
    "\n",
    "# Step 2: Read the third input (should find the first slot)\n",
    "query = x_samples[2]  # New input similar to the first\n",
    "similarities = cosine_similarity(query, M)  # Compute similarity with memory\n",
    "w_read = softmax(similarities)  # Compute read weights\n",
    "r_t = np.sum(w_read[:, None] * M, axis=0)  # Perform weighted read\n",
    "\n",
    "# Display memory, query, and read result using pandas\n",
    "df = pd.DataFrame(M, columns=[\"dim1\", \"dim2\", \"dim3\"])\n",
    "df[\"read_weight\"] = w_read\n",
    "\n",
    "# Show the DataFrame\n",
    "print(\"Simulated NTM Memory (Memory Slots and Read Weights):\\n\")\n",
    "print(df)\n",
    "\n",
    "# Output the read result\n",
    "print(\"\\nRead Result (r_t):\\n\", r_t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **\"Explanation for Dummies\"**\n",
    "A Neural Turing Machine (NTM)** is a **neural network** that uses an **external memory (a kind of notebook)** to quickly **remember information**.\n",
    "\n",
    "This memory is a table made up of **rows that store numerical data (called \"features\")**. When the **model** receives **new input**, it **compares this input to each row in the memory** using a measure called \"**cosine similarity**,\" which indicates **how similar two vectors are**.\n",
    "\n",
    "**After calculating the similarity**, the model assigns an **importance weight to each row (softmax)**. The **most similar** row will **have a higher weight**. Finally, the model combines all the weighted rows together to produce a final reading from the memory.\n",
    "Bur, having **limited memory (few slots)**, when you write a new feature into memory, typically this happens\n",
    "therefore inevitably there is a risk of **\"catastrophic forgetting\"**.\n",
    "\n",
    "In essence, the **NTM can learn quickly from just a few examples** because it **can immediately remember what it sees using the external memory**.\n",
    "\n",
    "To avoid this phenomenon, **Memory-Augmented Neural Network models (such as NTM)** adopt more advanced strategies, for example:\n",
    "\n",
    "They write to the lines that have been used least recently **(Least Recently Used - LRU)**.\n",
    "\n",
    "They **write to memory locations** chosen through **more sophisticated strategies** (such as gating mechanisms or adaptive writing), which **avoid simple direct replacement**.\n",
    "\n",
    "They **combine external memory** (fast and dynamic) with **internal memory** (parameters updated slowly via backpropagation) to **balance short-term and long-term memory**.\n",
    "\n",
    "In other words, a **real NTM avoids directly and brutally overwriting the most relevant information**, precisely to avoid falling into catastrophic forgetting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Least Recently Used Access (LRUA)**\n",
    "\n",
    "Previous **Neural Turing Machine (NTM)** models used **two modes to read and write to memory**:\n",
    "\n",
    "1. **Content-based**: Search for similarity of content.\n",
    "2. **Location-based**: Access based on location, similar to scrolling on a \"tape\".\n",
    "\n",
    "The **position-based mode was useful in sequential tasks** (predicting sequences), but it is **not ideal for tasks where position does not matter**, but combining information is essential (\"conjunctive coding\").\n",
    "\n",
    "We then introduce **Least Recently Used Access (LRUA)**, which uses **exclusively content-based access**.\n",
    "\n",
    "How it works:\n",
    "- **Write** to the **least used slot** in memory, thus **preserving recent information already stored**.\n",
    "- **Update** the most **recently used slot**, potentially **overwriting older information** with more relevant information.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is interpolation?**\n",
    "**Interpolation** is simply a way of **combining two values ​​with a certain weight**. \n",
    "In practice:\n",
    "- You **don't choose** option 1 completely or option 2 completely\n",
    "- You **take a little bit** of option 1 and a little bit of option 2, **in a certain proportion**\n",
    "\n",
    "In **mathematical terms**\n",
    "**Interpolation** can be expressed as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$result = α × option1 + (1-α) × option2$$\n",
    "\n",
    "Where **α (alpha) is a value between 0 and 1** that determines **how much weight to give to option 1 versus option 2**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of the **LRUA**, the **formula** would be something like:\n",
    "\n",
    "$$memory allocation = α × least used slot weight + (1-α) × most recent slot weight$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- If $α = 1$, always choose the **least used slot**\n",
    "- If $α = 0$, always choose the **most recent slot**\n",
    "- If $α = 0.3$, **give 30%** importance to the least used slot and **70%** to the most recent slot\n",
    "\n",
    "The value of α is **determined dynamically based** on memory access patterns and the needs of the model in the current context\n",
    "\n",
    "**Interpolation** is only used as a **decision mechanism** to choose between two strategies:\n",
    "\n",
    "- **Use** the **least recently used slot**\n",
    "- **Update** the **most recently used slot**\n",
    "\n",
    "Once the decision is made, the information is written entirely into the chosen slot, not split between multiple slots. It is more like deciding which post-it to write on, rather than splitting the message between multiple post-its."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Mathematical formulas used**\n",
    "\n",
    "**1. Calculate memory usage weights**:\n",
    "\n",
    "At each time step, update the memory \"usage weights\" (\\(w_t^u\\)), which track how often a slot has been used recently:\n",
    "\n",
    "$$\n",
    "w_t^u = \\gamma w_{t-1}^u + w_t^r + w_t^w\n",
    "\\tag{5}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $( \\gamma )$ is a decay parameter,\n",
    "- $( w_t^r )$ are the read weights,\n",
    "- $( w_t^w )$ are the write weights.\n",
    "\n",
    "---\n",
    "\n",
    "**2. Identify the least used slots**:\n",
    "\n",
    "To identify the least used slots, select the ones with the lowest usage weights. The function $(m(v,n))$ indicates the nth smallest value of the vector $(v)$:\n",
    "\n",
    "$$\n",
    "w_t^{lu}(i) =\n",
    "\\begin{cases}\n",
    "0, & \\text{if } w_t^u(i) > m(w_t^u, n) \\\\[6pt]\n",
    "1, & \\text{se } w_t^u(i) \\leq m(w_t^u, n)\n",
    "\\end{cases}\n",
    "\\tag{6}\n",
    "$$\n",
    "\n",
    "The parameter $$( n )$$ represents the number of reads from memory.\n",
    "\n",
    "---\n",
    "\n",
    "**3. Final calculation of write weights**:\n",
    "\n",
    "The write weights $(w_t^w)$ are calculated as a convex combination of the previous read weights and the previously calculated least used weights, using a learnable sigmoidal \"gate\":\n",
    "\n",
    "$$\n",
    "w_t^w = \\sigma(\\alpha) w_{t-1}^r + \\bigl(1 - \\sigma(\\alpha)\\bigr) w_{t-1}^{lu}\n",
    "\\tag{7}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $(\\sigma(\\alpha) = \\frac{1}{1 + e^{-\\alpha}})$ is a sigmoidal function,\n",
    "- $(\\alpha)$ is a learnable scalar parameter (gate).\n",
    "\n",
    "---\n",
    "\n",
    "**4. Writing to memory**:\n",
    "\n",
    "Before writing, the least used slot is zeroed. Then, the memory is updated with the calculated weights:\n",
    "\n",
    "$$\n",
    "M_t(i) = M_{t-1}(i) + w_t^w(i) k_t\n",
    "\\tag{8}\n",
    "$$\n",
    "\n",
    "Then, the new vector (feature) can be written to the newly emptied slot (the least used one), or the most recently used slot can be updated. If we choose the latter option, the least used memory is simply erased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Explanation for Dummies**\n",
    "\n",
    "- **External memory** is a table with slots (rows), each containing numerical data (the \"features\").\n",
    "\n",
    "- The **Least Recently Used Access (LRUA)** module works like this:\n",
    "- When new information (new \"features\") arrives to be stored, LRUA must decide **where to write it**.\n",
    "- To do this, it uses an interpolation (a weighted combination) between:\n",
    "1. The memory slot that has been used least recently (to save recent information without erasing it immediately).\n",
    "2. The memory slot used most recently (to possibly update information already stored with more relevant data).\n",
    "\n",
    "- This choice is made via a \"gate\" parameter (a port), which is automatically learned by the neural network itself during training.\n",
    "\n",
    "Then, the new information is written **either in the least used slot** (thus protecting recent memory), **or in the most recently used slot**, overwriting and updating that information.\n",
    "\n",
    "In practice, LRUA continuously balances the memory between new information and updating the information already present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature1</th>\n",
       "      <th>Feature2</th>\n",
       "      <th>Feature3</th>\n",
       "      <th>Usage Weights</th>\n",
       "      <th>Slot Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Most Used</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.868540</td>\n",
       "      <td>0.806575</td>\n",
       "      <td>0.660996</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Least Used</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.639077</td>\n",
       "      <td>0.450875</td>\n",
       "      <td>0.525840</td>\n",
       "      <td>0.5</td>\n",
       "      <td>Intermediate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Feature1  Feature2  Feature3  Usage Weights   Slot Status\n",
       "0  0.100000  0.700000  0.300000            1.0     Most Used\n",
       "1  0.868540  0.806575  0.660996            0.2    Least Used\n",
       "2  0.639077  0.450875  0.525840            0.5  Intermediate"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ===================================\n",
    "# Simplified LRUA Practical Example\n",
    "# ===================================\n",
    "\n",
    "# Memory: 3 slots, each with 3 features\n",
    "mem_slots = 3\n",
    "mem_dim = 3\n",
    "\n",
    "# Initialize random memory for simulation\n",
    "M = np.random.rand(mem_slots, mem_dim)\n",
    "\n",
    "# Initial usage weights (simulate how much each slot is used)\n",
    "usage_weights = np.array([0.9, 0.2, 0.5])  # slot 0 heavily used, slot 1 rarely used\n",
    "\n",
    "# New feature to store\n",
    "new_feature = np.array([0.1, 0.7, 0.3])\n",
    "\n",
    "# Least recently used and most recently used slots\n",
    "least_used_slot = np.argmin(usage_weights)   # least used (slot with the lowest weight)\n",
    "most_used_slot = np.argmax(usage_weights)    # most used (slot with the highest weight)\n",
    "\n",
    "# Sigmoid gate (learned by the model, simulated here)\n",
    "alpha = 0.4\n",
    "gate = 1 / (1 + np.exp(-alpha))\n",
    "\n",
    "# Interpolation between the chosen slots:\n",
    "# if gate is close to 1 -> preference for updating the most recently used slot\n",
    "# if gate is close to 0 -> preference for using the least used slot\n",
    "write_weights = gate * most_used_slot + (1 - gate) * least_used_slot\n",
    "selected_slot = round(write_weights)\n",
    "\n",
    "# Write the new feature into memory\n",
    "M[selected_slot] = new_feature\n",
    "\n",
    "# Update the usage weight\n",
    "usage_weights[selected_slot] = 1.0  # just used, so maximum usage\n",
    "\n",
    "# Final memory visualization\n",
    "df_mem = pd.DataFrame(M, columns=[\"Feature1\", \"Feature2\", \"Feature3\"])\n",
    "df_mem[\"Usage Weights\"] = usage_weights\n",
    "df_mem[\"Slot Status\"] = [\"Most Used\" if i == most_used_slot else \"Least Used\" if i == least_used_slot else \"Intermediate\" for i in range(mem_slots)]\n",
    "\n",
    "df_mem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We have **3 slots** of memory, each with 3 features.\n",
    "- A **new feature** has been stored.\n",
    "- By interpolation (gate = 0.6), the **most recently used slot** has been selected for updating, because the gate value favored this option.\n",
    "\n",
    "**Final memory result:**\n",
    "\n",
    "- **Slot 0 (\"Most Used\")**: updated with the new feature `[0.1, 0.7, 0.3]`, maximum usage weight (1.0).\n",
    "- **Slot 1 (\"Least Used\")**: unchanged, lowest usage (0.2).\n",
    "- **Slot 2 (\"Intermediate\")**: unchanged, medium usage (0.5)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results show that the **Memory-Augmented Neural Network (MANN)** model with the **Least Recently Used Access (LRUA)** module is extremely effective for rapid learning tasks (**one-shot learning**):\n",
    "\n",
    "- It **learns new classes quickly** and with high accuracy.\n",
    "- It significantly outperforms human performance.\n",
    "- It outperforms conventional models even with complex labels and many classes per episode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key findings of the study:\n",
    "- Many real-world **problems** require **fast learning** capabilities based on **few examples**. These problems are a challenge for classical deep learning, which usually learns slowly through gradual updates.\n",
    "- This study approaches the problem using the concept of **meta-learning**: gradual and general learning across different tasks (\"background knowledge\"), combined with a flexible and fast memory to remember specific information from new tasks.\n",
    "\"Learning to learn\".\n",
    "- The main innovation proposed is a particular type of neural network with external memory (**Memory-Augmented Neural Network - MANN**) that is particularly effective for meta-learning. This memory is **separated from the network structure that controls the processes**.\n",
    "\n",
    "### Results obtained:\n",
    "- The proposed MANN clearly outperformed a traditional **LSTM** in both **classification** and **regression** tasks based on few examples.\n",
    "- The tasks studied require not only **remembering information**, but also **generalizing** (transferring previous knowledge to new examples), a skill called **\"inductive transfer\"**.\n",
    "- MANNs were **very well suited** for these tasks thanks to the combination of a flexible **external memory** and the powerful learning capabilities of deep neural networks.\n",
    "\n",
    "### Comparison with human learning:\n",
    "- **Meta-learning** is considered a key component of human intelligence.\n",
    "- In an informal comparison with human subjects, the *MANN** showed superior performance, even with amounts of information that could easily be managed by human working memory.\n",
    "- However, when the memory was not emptied between different tasks (episodes), the MANN showed phenomena of **proactive interference**, similar to those observed in human memory (difficulty in remembering new information due to old information).\n",
    "\n",
    "## Short final summary:\n",
    "- This study confirms that **MANNs** are effective for **meta-learning** with few examples.\n",
    "- They show great **generalization** ability and can represent a **valid model for human learning**.\n",
    "- Interesting problems remain open, such as further optimizing **memory strategies**, exploring a greater variety of tasks and addressing the challenge of active learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
