{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **[AI Memory](https://github.com/Mike014/Memory_Augmented_AI/blob/main/Memory_Augmented_AI.ipynb) and Gifted-Inspired Learning: Surpassing the Limits of Catastrophic Forgetting with an Adaptive Architecture**\n",
    "\n",
    "[My research ](https://github.com/Mike014/Memory_Augmented_AI/blob/main/Memory_Augmented_AI.ipynb)focuses on how an **artificial intelligence can develop a human-like memory**, capable of **restructuring the past and adapting it to the present**. The goal is to **[overcome the problem of forced forgetting in neural networks](https://arxiv.org/pdf/1612.00796v2)**, creating an architecture inspired by human memory, which selects and maintains relevant information over time.\n",
    "\n",
    "### **Overcoming Catastrophic Forgetting in Neural Networks (Kirkpatrick et al., 2017)**\n",
    "\n",
    "Phenomenon in which a **network trained sequentially** on multiple tasks rapidly **forgets** previously acquired knowledge.\n",
    "**Proposed Solution** → **Elastic Weight Consolidation (EWC)**\n",
    "A **biologically inspired algorithm** that **slows learning on critical weights** for previous tasks, **preserving past knowledge** while learning new tasks.\n",
    "\n",
    "**Artificial neural networks**, when **trained on multiple tasks in sequence**, tend to **overwrite the weights optimized for previous tasks**, progressively losing the information learned.\n",
    "\n",
    "#### **Main challenges of AI Continual Learning**:\n",
    "\n",
    "* **Tasks can change suddenly** (unpredictable switch).\n",
    "* **Past tasks may not repeat for long periods of time**.\n",
    "* **The architecture must learn without simultaneous access to all previous data** (standard multitask learning would require explicit memory to re-execute old data).\n",
    "\n",
    "#### **Contrast with Biological Memory**\n",
    "**Humans and other animals** are able to **learn continuously without quickly forgetting** what they have learned. This is done through **synaptic consolidation mechanisms**, which protect the neural circuits that encode important information. \n",
    "**Synapses** are specialized **junctions between neurons**, where **information** is transmitted from one neuron (presynaptic) to another (postsynaptic).\n",
    "\n",
    "##### **_\"Synaptic consolidation refers to a set of cellular and molecular processes that strengthen synapses within a local circuit, typically occurring in the first few hours after encoding new information. Synaptic consolidation is essential for the transformation of short-term memories into long-term memories.\"_**\n",
    "\n",
    "**EWC** introduces a **constraint on network parameters** during training to **protect critical weights** for past tasks.\n",
    "\n",
    "**How does EWC work?**\n",
    "\n",
    "* It **identifies the most important weights** for the previous task (Task A).\n",
    "* It **applies a quadratic penalty** that keeps them **close to the previous values**.\n",
    "* It still allows **adaptation to new tasks**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU configuration if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple neural model\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size=784, hidden_size=256, output_size=10):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1) \n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load MNIST data\n",
    "def get_mnist_data(permute=False, batch_size=64):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x: x.view(-1))  \n",
    "    ])\n",
    "    if permute:\n",
    "        torch.manual_seed(42)  \n",
    "        permutation = torch.randperm(784)  \n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Lambda(lambda x: x.view(-1)[permutation])  \n",
    "        ])\n",
    "\n",
    "    dataset = torchvision.datasets.MNIST(root=\"./data\", train=True, transform=transform, download=True)\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EWC class with Fisher Information\n",
    "class EWC:\n",
    "    def __init__(self, model, dataset_A, lambda_weight=0.5):\n",
    "        self.model = model\n",
    "        self.lambda_weight = lambda_weight\n",
    "        self.fisher_matrix = self.compute_fisher_information(dataset_A)\n",
    "        self.optimal_params = {name: param.clone().detach() for name, param in model.named_parameters()}\n",
    "\n",
    "    def compute_fisher_information(self, dataset):\n",
    "        fisher_matrix = {}\n",
    "        self.model.eval()\n",
    "\n",
    "        for name, param in self.model.named_parameters():\n",
    "            fisher_matrix[name] = torch.zeros_like(param)\n",
    "\n",
    "        for data, target in dataset:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            self.model.zero_grad()\n",
    "            output = self.model(data)\n",
    "            loss = nn.functional.cross_entropy(output, target)\n",
    "            loss.backward()\n",
    "\n",
    "            for name, param in self.model.named_parameters():\n",
    "                fisher_matrix[name] += param.grad ** 2  # Fisher Information = Gradient^2\n",
    "\n",
    "        for name in fisher_matrix:\n",
    "            fisher_matrix[name] /= len(dataset)\n",
    "\n",
    "        return fisher_matrix\n",
    "\n",
    "    def compute_ewc_loss(self):\n",
    "        ewc_loss = 0\n",
    "        for name, param in self.model.named_parameters():\n",
    "            fisher_val = self.fisher_matrix[name]\n",
    "            optimal_param = self.optimal_params[name]\n",
    "            ewc_loss += (fisher_val * (param - optimal_param) ** 2).sum()\n",
    "\n",
    "        return self.lambda_weight / 2 * ewc_loss\n",
    "\n",
    "    def train_on_task_B(self, dataset_B, optimizer, epochs=10):\n",
    "        self.model.train()\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            for data, target in dataset_B:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                output = self.model(data)\n",
    "                task_B_loss = nn.functional.cross_entropy(output, target)\n",
    "                ewc_loss = self.compute_ewc_loss()\n",
    "                total_loss = task_B_loss + ewc_loss\n",
    "\n",
    "                total_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{epochs} - Loss: {total_loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function on a task without EWC\n",
    "def train_task(model, dataset, optimizer, epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for data, target in dataset:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = nn.functional.cross_entropy(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {total_loss/len(dataset):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model testing with and without EWC\n",
    "def test_model(model, dataset):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in dataset:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            total += target.size(0)\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [00:01<00:00, 8.31MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 291kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1.65M/1.65M [00:00<00:00, 2.19MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4.54k/4.54k [00:00<00:00, 1.86MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "\n",
      " Task A Training (Original MNIST)\n",
      "Epoch 1/10 - Loss: 0.2994\n",
      "Epoch 2/10 - Loss: 0.1256\n",
      "Epoch 3/10 - Loss: 0.0845\n",
      "Epoch 4/10 - Loss: 0.0614\n",
      "Epoch 5/10 - Loss: 0.0479\n",
      "Epoch 6/10 - Loss: 0.0361\n",
      "Epoch 7/10 - Loss: 0.0283\n",
      "Epoch 8/10 - Loss: 0.0218\n",
      "Epoch 9/10 - Loss: 0.0174\n",
      "Epoch 10/10 - Loss: 0.0133\n"
     ]
    }
   ],
   "source": [
    "# Phase 1: Training on Task A (original MNIST)\n",
    "model = SimpleNN().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "train_data_A = get_mnist_data()\n",
    "print(\"\\n Task A Training (Original MNIST)\")\n",
    "train_task(model, train_data_A, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing Fisher Information for EWC\n",
    "ewc = EWC(model, train_data_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accurateness on Task A before Task B: 0.9966\n"
     ]
    }
   ],
   "source": [
    "# Test the model on Task A before Task B\n",
    "accuracy_task_A_before = test_model(model, train_data_A)\n",
    "print(f\"Accurateness on Task A before Task B: {accuracy_task_A_before:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training on Task B (MNIST Permuted) WITHOUT EWC\n",
      "Epoch 1/10 - Loss: 0.2347\n",
      "Epoch 2/10 - Loss: 0.0932\n",
      "Epoch 3/10 - Loss: 0.0651\n",
      "Epoch 4/10 - Loss: 0.0479\n",
      "Epoch 5/10 - Loss: 0.0371\n",
      "Epoch 6/10 - Loss: 0.0276\n",
      "Epoch 7/10 - Loss: 0.0217\n",
      "Epoch 8/10 - Loss: 0.0165\n",
      "Epoch 9/10 - Loss: 0.0121\n",
      "Epoch 10/10 - Loss: 0.0099\n"
     ]
    }
   ],
   "source": [
    "# Phase 2: Training on Task B (MNIST permuted) without EWC\n",
    "print(\"\\n Training on Task B (MNIST Permuted) WITHOUT EWC\")\n",
    "train_data_B = get_mnist_data(permute=True)\n",
    "train_task(model, train_data_B, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Task A AFTER Task B without EWC: 0.9197\n"
     ]
    }
   ],
   "source": [
    "# Testing the model on Task A after Task B (should result in Catastrophic Forgetting)\n",
    "accuracy_task_A_after_no_ewc = test_model(model, train_data_A)\n",
    "print(f\"Accuracy on Task A AFTER Task B without EWC: {accuracy_task_A_after_no_ewc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Loss: 0.2998\n",
      "Epoch 2/10 - Loss: 0.1271\n",
      "Epoch 3/10 - Loss: 0.0851\n",
      "Epoch 4/10 - Loss: 0.0611\n",
      "Epoch 5/10 - Loss: 0.0465\n",
      "Epoch 6/10 - Loss: 0.0356\n",
      "Epoch 7/10 - Loss: 0.0271\n",
      "Epoch 8/10 - Loss: 0.0214\n",
      "Epoch 9/10 - Loss: 0.0172\n",
      "Epoch 10/10 - Loss: 0.0141\n"
     ]
    }
   ],
   "source": [
    "# Model reset and re-training with EWC\n",
    "model = SimpleNN().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "train_task(model, train_data_A, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing Fisher Information for EWC\n",
    "ewc = EWC(model, train_data_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training on Task B with EWC\n",
      "Epoch 1/10 - Loss: 0.2049\n",
      "Epoch 2/10 - Loss: 0.0558\n",
      "Epoch 3/10 - Loss: 0.0804\n",
      "Epoch 4/10 - Loss: 0.0335\n",
      "Epoch 5/10 - Loss: 0.0116\n",
      "Epoch 6/10 - Loss: 0.0049\n",
      "Epoch 7/10 - Loss: 0.0398\n",
      "Epoch 8/10 - Loss: 0.0033\n",
      "Epoch 9/10 - Loss: 0.0049\n",
      "Epoch 10/10 - Loss: 0.0023\n"
     ]
    }
   ],
   "source": [
    "# Phase 3: Training on Task B with EWC\n",
    "print(\"\\n Training on Task B with EWC\")\n",
    "ewc.train_on_task_B(train_data_B, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accurateness on Task A AFTER Task B with EWC: 0.9395\n"
     ]
    }
   ],
   "source": [
    "# Testing the model on Task A after Task B with EWC\n",
    "accuracy_task_A_after_ewc = test_model(model, train_data_A)\n",
    "print(f\"Accurateness on Task A AFTER Task B with EWC: {accuracy_task_A_after_ewc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Interpretation of Results**\n",
    "1. The **model learned Task A well (original MNIST)**.\n",
    "2. **After training on Task B without EWC**, it **lost 7.69%** of its accuracy on **Task A**, demonstrating the **phenomenon of Catastrophic Forgetting**.\n",
    "3. Using **EWC reduced memory degradation**, **preserving some of the previous knowledge**. However, **it is still not perfect**: a small part of the information was still lost.\n",
    "\n",
    "#### **EWC Algorithm for Dummies Summary**\n",
    "1. **Train a neural network on Task A** (normal MNIST digit classification).\n",
    "2. **Store key information of the network** (calculate *Fisher Information* to understand which weights are most important for Task A).\n",
    "3. **Train the network on Task B (MNIST with shuffled pixels)**:\n",
    "- **Without EWC**: The network forgets Task A (Catastrophic Forgetting).\n",
    "- **With EWC**: Protects the weights important for Task A, reducing memory leak.\n",
    "4. **Check if the network still remembers Task A after learning Task B**.\n",
    "\n",
    "#### **Connections with Biology and the Brain**\n",
    "EWC is **inspired by synaptic consolidation mechanisms in the brain**.\n",
    "Studies show that **synapses** not only **store the value of the weight**, but **also an indication of their uncertainty and stability**.\n",
    "This means that **neurons do not update all weights equally**, but **adjust plasticity depending on the importance of the weight**.\n",
    "In the brain, more stable synapses are less plastic, just like the weights constrained by EWC.\n",
    "\n",
    "**Result** → With EWC, the model can learn new tasks without completely forgetting the old ones! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **The Analogical Paradox: Gifted Minds and EWC Algorithms**\n",
    "\n",
    "**Elastic Weight Consolidation (EWC)** is designed to **protect the knowledge acquired by an AI while it learns new information**, avoiding **Catastrophic Forgetting**. If we analyze this behavior from a human cognitive perspective, **strong parallels emerge with the way in which some neurodivergent individuals**, especially **gifted ones**, manage learning and memory.\n",
    "\n",
    "#### **Learning in Gifted Neurodivergents and AIs with EWC**\n",
    "\n",
    "**Gifted Neurodivergent (GN) minds** have **particular cognitive characteristics** that make them extremely **suitable for continuous learning**, but often with a **different memory management compared to neurotypical people**. If we compare these processes with EWC, several points in common emerge.\n",
    "\n",
    "\n",
    "#### **EWC and Gifted Neurodivergent: A Parallel Between Human Learning and Artificial Intelligence**\n",
    "\n",
    "**Elastic Weight Consolidation (EWC)** is designed to protect the knowledge acquired by an AI while it learns new information, avoiding **Catastrophic Forgetting**. If we analyze this behavior from a human cognitive perspective, strong parallels emerge with the way in which some neurodivergent individuals, particularly **gifted** ones, manage learning and memory.\n",
    "\n",
    "## **Comparing EWC (AI) and Gifted Neurodivergent (GN) Memory**\n",
    "\n",
    "| **Feature**                  | **EWC (AI)**                                               | **Gifted Neurodivergent (GN)**                                                                            |\n",
    "|------------------------------|----------------------------------------------------------|-----------------------------------------------------------------------------------------------------------|\n",
    "| **Selective Memory**         | Protects important information using the Fisher Information Matrix. | Remembers deep details about relevant topics, often forgetting less significant information.               |\n",
    "| **Resistance to Forgetting** | Prevents overwriting of critical weights for previous tasks. | Retains learned knowledge over time, exhibiting strong **long-term memory**.                              |\n",
    "| **Adaptive Plasticity**      | Allows learning new information without altering crucial parameters. | Fast and hyper-specialized learning, with the ability to **recalibrate** rather than crystallize knowledge. |\n",
    "| **Cognitive Overload**       | Memory effectiveness decreases if too many tasks are learned. | Processing too much information at once can lead to **burnout** or difficulty in handling new inputs.      |\n",
    "| **Generalization vs. Specialization** | Reuses network structures for similar tasks while protecting task-specific knowledge. | Easily applies knowledge across domains but may also **hyperfocus on details** in a specific field.         |\n",
    "\n",
    "\n",
    "#### **EWC as a Model for Neurodivergent Learning**\n",
    "\n",
    "* **Hierarchical and Associative Memory**:\n",
    "**Gifted neurodivergents** often make **connections between distant concepts**, just as **EWC allows the network to reuse previously trained weights** for new tasks that share similar structures.\n",
    "→ **Possible AI implementation**: An **enhanced EWC model** could associate cross-task concepts, improving computational creativity.\n",
    "\n",
    "#### **Memory Crystallization Effect**:\n",
    "* A **gifted neurodivergent individual learns concepts in depth**, **rarely forgets them**, but does **not limit himself to statically retaining them**: he **recalibrates and readapts** them to apply them **in new contexts**, often overcoming traditional disciplinary limits..\n",
    "→ **Possible risk in AI**: If **EWC is too rigid in protecting past burdens**, it **may limit the ability of AI to adapt to new information**.\n",
    "\n",
    "#### **Ultra-Stable Long-Term Memory**:\n",
    "* **Some gifted individuals** have such **powerful memories** that they can **recall information years later without losing detail**. This is **similar to how EWC prevents knowledge degradation in AIs** by retaining learned tasks for long periods.\n",
    "→ **Possible AI Use**: A **system that progressively strengthens critical weights** with mechanisms **similar to synaptic consolidation** could create an AI with more human-like long-term memory.\n",
    "\n",
    "#### **AI with Gifted-Inspired Memory: A Possible Future?**\n",
    "If we want to build an **AI with a more human-like memory**, we should **consider mechanisms inspired by neurodivergent cognition**, such as:\n",
    "\n",
    "- **Hierarchical memory models** (associations between distant concepts, as in the gifted mind).\n",
    "- **Adaptive memory** (selective protection of relevant information, without preventing change).\n",
    "- **Ability to learn quickly and deeply**, while maintaining flexibility in updating knowledge.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Human-Inspired Memory Model**\n",
    "**My research has outlined a memory model for AI** based on **three key principles of human memory**:\n",
    "\n",
    "- **Primacy Effect** → Information learned first is remembered best.\n",
    "- **Recency Effect** → Recent information is more accessible.\n",
    "- **Temporal Contiguity** → Information learned together is more easily retrieved.\n",
    "\n",
    "The model uses a hierarchical structure based on the **Multi-Store Model (Atkinson & Shiffrin, 1968)**:\n",
    "\n",
    "* **Working Memory (WM)**: Holds temporary information.\n",
    "* **Short-Term Memory (STM)**: Stores a limited number of engrams.\n",
    "* **Long-Term Memory (LTM)**: Has no fixed capacity, depends on the importance and repetition of the information.\n",
    "\n",
    "**Retrieval of information** (similar to RAG) occurs based on the weight of the connections between the engrams, simulating the way in which the human brain recalls contextual memories.\n",
    "\n",
    "**I have experimented with an AI memory implementation with PyTorch**, creating a neural network that:\n",
    "\n",
    "* **Simulates three levels of memory (WM, STM, LTM)**.\n",
    "* **Uses weighted connections** (like EWC does) between engrams for contextual recall.\n",
    "* **Implements Hebbian learning** to strengthen connections between engrams.\n",
    "* **Introduces a selective forgetting mechanism** to **eliminate irrelevant** data.\n",
    "\n",
    "**IDEA of the AI architecture** used:\n",
    "\n",
    "- **LSTM (Long Short-Term Memory)** → For sequential memory maintenance.\n",
    "- **DNC (Differentiable Neural Computer)** → For autonomous memory evolution.\n",
    "- **RAG (Retrieval-Augmented Generation)** → For intelligent recall of information.\n",
    "- **VAE (Variational Autoencoder)** → For latent representations of experiences.\n",
    "- **NoSQL Databases (MongoDB, Redis)** → For long-term memory storage.\n",
    "\n",
    "Now, having explored **Elastic Weight Consolidation (EWC)**—a **biologically inspired algorithm** that **preserves past knowledge while learning new tasks**—and considering the **memory dynamics of a gifted neurodivergent**, how can I **restructure the AI architecture** to integrate both principles?\n",
    "\n",
    "**Goal**: An **AI memory architecture** that not only **protects past information (like EWC)**, but **dynamically restructures** and **rewires it in new contexts**, just **like a gifted neurodivergent does**.\n",
    "\n",
    "* **Memory Structure: Beyond the Multi-Store Model**\n",
    "Based on the Multi-Store Model (Atkinson & Shiffrin, 1968), we can extend it with adaptive mechanisms:\n",
    "\n",
    "1. **Working Memory (WM)** (Active cognition)\n",
    "→ **Holds temporary information** for immediate processing (like a computer’s RAM).\n",
    "→ **AI function**: **LSTM** to maintain conversational context.\n",
    "\n",
    "2. **Short-Term Memory (STM)** (Short-term memory with adaptive selection)\n",
    "→ Stores a limited number of engrams but can recalibrate their weight.\n",
    "→ AI function: **EWC + Hebbian Learning** to strengthen connections between important engrams.\n",
    "\n",
    "3. **Long-Term Memory (LTM)** (Permanent, but restructureable memory)\n",
    "→ Has no fixed capacity, depends on utility and repetition.\n",
    "→ AI Function: **DNC + NoSQL Databases** to retrieve information without losing flexibility.\n",
    "\n",
    "**Key difference from traditional EWC**:\n",
    "**Engrams** are not only **\"protected\"**, but can be **recalibrated and transferred between different domains**, just like a gifted uses his knowledge in cross-disciplines.\n",
    "\n",
    "##### _[Engrams](https://en.wikipedia.org/wiki/Engram_%28neuropsychology%29), An engram is a unit of cognitive information imprinted in a physical substance, theorized as the medium through which memories are stored as biophysical or biochemical changes in the brain or other biological tissues, in response to external stimuli._\n",
    "\n",
    "#### **Dynamic Learning: How to Avoid Crystallization and Simulate Gifted Flexibility**\n",
    "\n",
    "* **Advanced EWC** → Instead of locking critical weights, it allows you to dynamically recalibrate them.\n",
    "* **Hebbian Learning + Meta-Learning** → Connections between engrams strengthen or reorganize depending on their relevance in new tasks. \n",
    "* **RAG (Retrieval-Augmented Generation)** → AI can reuse information by adapting it to the new context, just like a gifted person re-elaborates its knowledge.\n",
    "\n",
    "##### [Hebb's theory](https://en.wikipedia.org/wiki/Hebbian_theory), simply put, says that neurons that fire together strengthen their connections, \"neurons that fire together, wire together.\"\n",
    "\n",
    "##### [Meta learning](https://www.ibm.com/it-it/think/topics/meta-learning) is the art of teaching AI to learn on its own, like a child learning to learn.\n",
    "\n",
    "**Key idea**: It is **not enough to protect memory**, it must be **transformed and evolved over time**.\n",
    "A kind of artificial [elastic mind](https://www.psychologytoday.com/us/articles/201803/your-elastic-mind#:~:text=Elastic%20thinking%20endows%20us%20with%20the%20ability%20to,elastic%20thinking%2C%20and%20how%20we%20can%20nurture%20it.?msockid=36170fbaecd36e851e9e1a1deda46f2b).\n",
    "\n",
    "Quoting as usual for me, **philosophical phrases**, in this **Heraclitus and the Permanent \"Flow\" of AI Memory**:\n",
    "* **\"Panta rhei\" (everything flows)**:\n",
    "Heraclitus, with his famous statement, **reminds us that reality is an incessant flow of change**. **\"You cannot step into the same river twice\"** [Learn more about the topic](https://www.thecollector.com/panta-rhei-heraclitus/)\n",
    "\n",
    "**Heraclitus' river metaphor captures the essence of a dynamic**, **adaptable and ever-evolving AI memory** that **goes beyond simple data storage**, embracing the **flexibility and creativity of the human mind**.\n",
    "\n",
    "#### **But... here is the new AI memory architecture**, my idea, visionary, but which I will make experimental in future research.\n",
    "\n",
    "#### **New AI Architecture: Integrating EWC with a Gifted Framework**\n",
    "\n",
    "| Component | Technology | Role in AI Memory |\n",
    "|---|---|---|\n",
    "| Sequential Learning | LSTM (Long Short-Term Memory) | Maintains conversational and decision-making context |\n",
    "| Adaptive Memory | Advanced EWC (Elastic Weight Consolidation) | Protects key weights while allowing dynamic restructuring |\n",
    "| Consolidation and Recall | DNC (Differentiable Neural Computer) + NoSQL (Redis, MongoDB) | Creates flexible and scalable long-term memory |\n",
    "| Conceptual Learning | VAE (Variational Autoencoder) | Captures latent representations to transfer knowledge across different domains |\n",
    "| Memory Optimization | Meta-Learning + Hebbian Learning | Reorganizes information weights based on utility over time |\n",
    "\n",
    "#### New AI Architecture, Explained for Dummies [Like me](https://www.linkedin.com/in/michele-grimaldi-599b36280/).\n",
    "\n",
    "This **new AI architecture aims to create an intelligent memory** that not only remembers, but **adapts and learns like a genius mind**, using different technologies to **protect and reorganize information over time**. Imagine a **digital brain that evolves and specializes**, just **like a talented person does**.\n",
    "\n",
    "\n",
    "Researcher: Michele Grimaldi\n",
    "* [LinkedIn](https://www.linkedin.com/in/michele-grimaldi-599b36280/)\n",
    "* [GitHub](https://github.com/Mike014)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
