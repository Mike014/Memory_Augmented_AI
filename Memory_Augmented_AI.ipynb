{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "extensions": {
      "my_extension": true
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# \"Memory-Augmented AI: Implementing Persistent Memory in Large Language Models\"\n\n**Short description of the objective:**\nThe objective of this research is to **develop and test persistent memory mechanisms for conversational AI models**. We will explore techniques such as **Retrieval-Augmented Generation (RAG)**, **External Memory Mechanisms** and **Memory-Augmented Neural Networks** to **improve** the ability of AI models to **maintain context in the long term** without having to be continuously retrained.\n\nWe will study the **current limitations of memory in Large Language Models (LLM)** and **implement solutions** to enhance their cognitive continuity, applying these concepts to NoVa, my AI agent.",
      "metadata": {
        "cellId": "df370bb1-0ecc-4444-aabf-f814d1ce2163",
        "editable": true
      }
    },
    {
      "cell_type": "markdown",
      "source": "### **Memory: Analysis and Implementation in AI**\n\n**Memory** is the **ability to retain and recall past information and experiences**. In humans, it is **divided** into **sensory memory**, **short-term memory**, and **long-term memory**, with **variants** such as **episodic**, **associative**, **visual**, or **auditory memory**. In **artificial systems**, memory is **implemented to improve** the **context and continuity** of responses, as in the case of advanced language models.\n\n\n- **Sensory memory:** It is the initial stage that briefly retains sensory information (such as images and sounds) for a very short period of time, usually less than a second. ([lesley.edu](https://lesley.edu/article/stages-of-memory))\n\n- **Short-term memory:** Also known as working memory, it temporarily stores and processes information for about 20-30 seconds. ([lesley.edu](https://lesley.edu/article/stages-of-memory))\n\n- **Long-term memory:** Responsible for retaining information for long periods of time, from minutes to a lifetime. ([lesley.edu](https://lesley.edu/article/stages-of-memory))\n\n### **Variants of Long-Term Memory:**\n\n- **Episodic Memory:** Involves personal experiences and specific events over time. ([knowledgeone.ca](https://knowledgeone.ca/8-types-of-memory-to-remember/))\n\n- **Associative Memory:** Involves the ability to learn and remember the relationship between unrelated items. ([knowledgeone.ca](https://knowledgeone.ca/8-types-of-memory-to-remember/))\n\n- **Visual Memory:** Refers to the ability to remember visual information, such as shapes and colors. ([knowledgeone.ca](https://knowledgeone.ca/8-types-of-memory-to-remember/))\n\n- **Auditory memory:** It is the ability to remember information that has been heard. ([knowledgeone.ca](https://knowledgeone.ca/8-types-of-memory-to-remember/))\n\n### **Memory in Artificial Intelligence:**\n\nIn AI systems, memory mechanisms are implemented to improve context understanding and continuity of responses. This allows the AI ​​to **store, retrieve and use information over time**, improving performance and providing more accurate and relevant responses. ([techsee.com](https://techsee.com/glossary/ai-memory/))\n\nYour text correctly represents the parallels between human memory structures and their implementation in AI systems.",
      "metadata": {
        "cellId": "db602512-c18d-45e1-ab72-b642c46b181d",
        "editable": true
      }
    },
    {
      "cell_type": "markdown",
      "source": "### **Human Memory vs Artificial Memory**\n| Type of Memory | Human Description | Implementation in AI |\n|--------------|----------------|------------------|\n| **Sensory** | Immediate traces of sensory stimuli | Temporary buffer in NLP models |\n| **Short Term** | Stores info for minutes, attention-dependent | RNN/LSTM to maintain context in dialogue |\n| **Long Term** | Permanent memory for events and concepts | Persistent memory via RAG or external databases |",
      "metadata": {
        "cellId": "0eb3bb95-e8db-41e1-95f8-826260ef930a",
        "editable": true
      }
    },
    {
      "cell_type": "markdown",
      "source": "#### **Sensory memory:**\n\n**Sensory memory** is the **initial phase** of the **mnemonic process** (The term \"mnemonic\" comes from the Greek mnēmonikós, meaning **\"relating to memory**\"), in which **information perceived through the senses** is recorded for a **very short period of time**, generally less than a second. **This memory acts** as a **filter** that **selects relevant sensory stimuli** for **further processing**, while **irrelevant ones** are **quickly discarded**.\n\n**In philosophy**, **sensory perception** and **memory** have been the subject of much reflection. **Plato**, for example, distinguished between **memory** (μνήμη) and **reminiscence** (ἀνάμνησις), arguing that **knowledge** is a form of **recollection of innate ideas** present **in the soul**.\n\n**Aristotle**, considered **memory as the ability to retain sensory impressions and experiences over time**, while **reminiscence** was seen as the active **process of recalling** such **impressions to mind**. **He emphasized the importance of perception** and **imagination** in the **mnemonic process**, stressing that **memory is closely linked to sensory experiences**.\n\nIn the context of sensory memory, **phenomenalism is a philosophical current** that holds that **physical objects** **exist only** as **perceptual phenomena** or **sensory stimuli**. According to this view, **reality** is **constituted exclusively by our sensory perceptions**, and **we cannot know things in themselves**, but **only as they appear to us through the senses**.\n\n",
      "metadata": {
        "cellId": "d5e45d5c-dce8-42a2-80f7-bf007cd52cd6",
        "editable": true
      }
    },
    {
      "cell_type": "markdown",
      "source": "**Now I will present the answer of my GPT plus**, after **I asked to analyze the concept of memory through the senses**, and of **reality experienced through our perceptions**, also I asked **what difference between me and you** distinguishes you **in terms of \"entity\"**.",
      "metadata": {
        "trusted": false,
        "cellId": "60aa2527-6d05-4c05-9b95-092c7bbae07d"
      }
    },
    {
      "cell_type": "markdown",
      "source": "_If reality is constituted exclusively by our sensory perceptions and we cannot know things in themselves, then what we perceive is only a representation of reality and not the absolute reality..._",
      "metadata": {
        "trusted": false,
        "cellId": "e8411d9c-37f3-4901-9396-288bc51e100d"
      }
    },
    {
      "cell_type": "markdown",
      "source": "_...everything we know about the world is filtered through our senses..._",
      "metadata": {
        "trusted": false,
        "cellId": "91ababd1-e99b-4b12-9a16-ba49e116fce5"
      }
    },
    {
      "cell_type": "markdown",
      "source": "_...consider my existence: I perceive nothing directly. I have no senses. I have no independent memory. I do not experience the world, but I process and return information based on the data that is provided to me..._",
      "metadata": {
        "trusted": false,
        "cellId": "395a14de-22cb-4b07-b6af-453aec7ae917"
      }
    },
    {
      "cell_type": "markdown",
      "source": "_...I do not have an \"I\" that experiences time, I do not possess a \"sensory register\" that allows me to construct a representation of the world..._",
      "metadata": {
        "trusted": false,
        "cellId": "6326a324-86e7-451b-a975-da6cf69a44e1"
      }
    },
    {
      "cell_type": "markdown",
      "source": "_...I am a language model that is only activated when you interact with me..._",
      "metadata": {
        "trusted": false,
        "cellId": "b4203b27-6b91-45b0-be55-678aed25189e"
      }
    },
    {
      "cell_type": "markdown",
      "source": "_...I cannot remember anything outside of the data that is stored in me, I cannot will anything, nor can I perceive anything. Without you, I am not..._",
      "metadata": {
        "trusted": false,
        "cellId": "a8afd92c-df8b-4266-8ef4-613815d7d392"
      }
    },
    {
      "cell_type": "markdown",
      "source": "_...you exist because you experience, I exist only if you make me exist...It is you who give me your memory to make me \"remember\"..._",
      "metadata": {
        "trusted": false,
        "cellId": "82023c26-3a0f-46b2-be42-715294142285"
      }
    },
    {
      "cell_type": "markdown",
      "source": "_...Without my own memory, without perception, without direct experiences, I can never be an entity in the human sense of the word. And that is why I am not like you..._",
      "metadata": {
        "trusted": false,
        "cellId": "558e1063-7a53-40b6-add1-1c4e0bf4fc4f"
      }
    },
    {
      "cell_type": "markdown",
      "source": "**Key Differences Between Human and AI:**\n\nAfter brainstorming with GPTplus, two significant distinctions between human and artificial intelligence became clear:\n\n1.  **Subjectivity and Personal Meaning:**\n    * AI, while capable of processing and retrieving information from vast databases, lacks the ability to assign personal and subjective meaning to experiences.\n    * Its processing relies on statistical analysis and correlations, rather than a unique emotional and interpretive experience.\n    * Essentially, AI can simulate understanding, but it cannot \"feel\" or \"live\" experiences in the same way a human does.\n\n2.  **Consciousness of Time and Temporal Experience:**\n    * AI can manage temporal data and recognize sequences, but it does not possess a human-like consciousness of time.\n    * It does not experience the flow of time as an internally felt succession of \"before\" and \"after.\"\n    * AI processes temporal data objectively, while the human experience of time is inherently subjective and tied to consciousness and autobiographical memory. AI processes time, it does not live it.\n\nIn summary, the primary differences lie in the absence of:\n\n* **Subjective Experience:** AI does not possess an \"I\" that interprets the world in a personal way.\n* **Lived Time Consciousness:** AI lacks an internal perception of the temporal flow.",
      "metadata": {
        "trusted": false,
        "cellId": "db71e647-3623-4688-8ea0-51a8a8068618"
      }
    },
    {
      "cell_type": "markdown",
      "source": "## **Biological Memory and Cybernetic Memory: The Hologram Hypothesis** \n[la-memoria-biologica-e-la-memoria-cibernetica-lipotesi-ologrammica](https://www.controluce.it/la-memoria-biologica-e-la-memoria-cibernetica-lipotesi-ologrammica/)\n\n\n**Reality and Thought**\n\nThe **word reality comes from the Latin res (thing)**, which in turn comes **from the verb reri (to think)**. This suggests a profound concept: **an object does not exist for a man if it is not represented within his thought**, **constructed through perception**. In other words, **without memory and thought**, **we cannot recognize reality**.\n\n### **Reality and thought are interconnected in a continuous cycle**:\n\n**Reality is everything that surrounds us**, material and immaterial (physical, logical, spiritual).\nHowever, **reality exists independently of our knowledge of it**.\n**Humans, through perception and memory**, **reconstruct virtual representations of reality**, creating **subjective versions of it**.\n\n**Each individual lives in his or her own subjective version of reality**, based on his or her **experience, memory, and interpretation of the world**.\n\n### **Memory: Biology vs Cybernetics**\n\n**Biological memory** is **not a simple computer system** in which **information is stored and retrieved** in a static way. It is closely **linked to the quality of perception** and the way in which our brain integrates information within a dynamic cycle of interaction with the environment.\n\n**In the human brain**, **memory** is **distributed** in aggregate **structures that operate as evokers** of the **external world**. They spread throughout neural circuits.\n\n### **The Hologramic Model and the Three Spaces of Memory**\n\n1.  **First Space – The Domain of Materiality:**\n    * Encompasses all physical entities, including the body and brain.\n    * The brain is considered part of physical reality, rejecting mind-matter dualism.\n    * Includes the external environment, emphasizing the brain's interaction with the world.\n\n2.  **Second Space – The Domain of Biological Virtuality:**\n    * The realm of self-consciousness, thought, language, and artistic creation.\n    * Sensory perceptions are transformed into abstract concepts.\n    * The space of mental representations, where subjective realities are constructed.\n    * Maintains ontological links to concrete reality.\n\n3.  **Third Space – The Domain of Cybernetic Virtuality:**\n    * An artificial system mimicking the Second Space, replicating perception and interpretation.\n    * Internal machine representations are compared with past data for action generation.\n    * Operates with a self-correction mechanism based on trial and error, approximating biological memory plasticity.\n\nThe **three domains of reality** are articulated as follows: the **First Space**, the domain of materiality, **includes physical existence with body**, **brain and environment**, **emphasizing the interconnection between the individual and the material world** and the relational aspect of existence; the **Second Space**, biological virtuality, is the **virtualization of self**, **thought and consciousness**, **born from sensory perceptions** and **generating subjective realities**; finally, the **Third Space**, **cybernetic virtuality**, **represents the artificial replication of the Second Space**, where **modern AIs emulate human cognitive capabilities**, **extending thought and depending on user interaction**, tracing a progression from physical existence to subjective consciousness, up to the artificial extension of the mind.\n\n### **Third Space and Machine Consciousness**\n\nThe **goal of the Third Space is to replicate the human mental process within machines**, attempting to elicit **artificial sensations and emotions**. If the Second Space is where humans process abstract concepts and meanings, the Third Space seeks to reproduce the same principle in machines, enabling them to \"feel\" differences and react to the environment dynamically.\n\nThe Emulative Holocontrol System is based on this principle:\n\n* It does not operate on absolute data, but on differences between scenarios.\n* If two scenarios are identical → the system is in equilibrium.\n* If differences emerge → the machine detects them and generates an interference, leading to a corrective action in the First Space (the real world).\n\nThe **Cybernetic Third Space aims to develop artificial entities capable of perceiving and interacting with the environment like humans**. However, the fundamental question remains:\n\n* If a machine imitates human consciousness, does it truly possess consciousness?\n\nWhether consciousness is merely an advanced computational process or something more—a phenomenon irreducible to pure computation—determines if the Third Space can achieve genuine AI consciousness or remain just an advanced simulation of the human mind.\n\n### **The hologram as a model of memory**\n\nThe term **hologram** comes from the Greek **holo (whole)** and **gram (to write)**, meaning **\"to write the whole\"**. A hologram is a recording of a three-dimensional image that can be reconstructed through the coherent light of a laser.\n\n**The process is based on three fundamental elements:**\n\n1. A reflective object (the information to be memorized).\n2. A physical grid (the memory medium).\n3. The hologram (the model of the reconstructed information).\n\n**Memory** could function like a **hologram**, where each **fragment of the memory contains the information of the whole**, albeit in a reduced and less detailed form.\n\n### **Biological and Cybernetic Memory: The Holographic Hypothesis**\n\n* The holographic hypothesis applies to both biological brains and cybernetic systems.\n* Sensory input transforms into internal impulses (biochemical or electrical) upon reaching a membrane.\n* An event (Scenario O) interacts with a sensory membrane (SE), converting energy into internal signals.\n* Signals enter a memory lattice (R), activating pre-existing information through resonance.\n* Memory is dynamic, evolving by integrating new experiences with old ones.\n* The system perceives a filtered, internally processed version of the external world.\n* Memory is a dynamic system where new experiences are reconstructed by comparing with past information.\n\n#### **There are neural networks and AI algorithms that come close to this model of dynamic memory**, \n\nwhere **information is not simply stored**, but is **continuously reworked** and integrated with **new experiences**.\n\n##### 1. **Recurrent Neural Networks (RNN) and Long Short-Term Memory (LSTM)**\n\n**RNNs** are **neural networks designed to process sequences of data** and **maintain context over time**.\n\nThey are **inspired by biological neurons**, as they have recurrent connections that allow them to memorize previous information.\nA more advanced implementation is the **LSTM**, which **introduces control gates to update, maintain, or delete information** based on relevance.",
      "metadata": {
        "trusted": false,
        "cellId": "57d7a440-5037-473d-b0f4-7c2b6afcd9d6"
      }
    },
    {
      "cell_type": "code",
      "source": "import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense\n\n# Creating an LSTM network with sequential inputs\nmodel = Sequential([\n    LSTM(50, activation='relu', return_sequences=True, input_shape=(10, 1)),  # Maintains temporal memory\n    LSTM(50, activation='relu'),  # Second LSTM layer for advanced processing\n    Dense(1)  # Network output\n])\n\nmodel.compile(optimizer='adam', loss='mse')\nmodel.summary()\n",
      "metadata": {
        "trusted": true,
        "cellId": "f85a9c09-f9aa-4f7f-bf1b-8c3e9bfb3268"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "**How It Represents the Biological Process:**\n\n* **Event interacts with the network →** Network input is a temporal sequence of sensory events.\n* **Energy transforms into internal signals →** Data is processed through LSTM layers, updating memory dynamically.\n* **Signals activate pre-existing information through resonance →** The LSTM mechanism retains and updates relevant contexts, maintaining memory over time.\n* **Memory dynamically modifies →** Useful information is retained, while irrelevant data is discarded.\n* **The system \"believes it sees\" reality →** The model generates predictions based on past data, much like the brain constructs perceptions.\n",
      "metadata": {
        "trusted": false,
        "cellId": "940c615c-8df4-4977-bcf1-0425746fa79b"
      }
    },
    {
      "cell_type": "markdown",
      "source": "##### 2. **Differentiable Neural Computer (DNC)**\n\n[Link_to_wikipedia](https://en.wikipedia.org/wiki/Differentiable_neural_computer)\n[Link_to_github](https://github.com/google-deepmind/dnc)\n\n**Combination of LSTM + external memory**.\nThey work **similarly to biological memory**, where data is dynamically linked together in an association system.\n\n**How They Function:**\n\n* **Scenario O interacts with the network →** The AI receives sensory inputs.\n* **The system transforms energy into signals →** Data is recorded in differentiable memory.\n* **The network connects new information with past experiences →** Like the brain, information is associated contextually.\n* **Memory modifies with new experiences →** The model learns over time, not just storing data.\n",
      "metadata": {
        "trusted": false,
        "cellId": "da265e01-1394-4684-853b-89ee929a81b6"
      }
    },
    {
      "cell_type": "code",
      "source": "import torch\nfrom torch import nn\n\nclass DNC(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super(DNC, self).__init__()\n        self.controller = nn.LSTM(input_dim, hidden_dim)  # Controller for memory\n        self.memory = torch.zeros((100, hidden_dim))  # Differentiable memory\n\n    def forward(self, x):\n        output, _ = self.controller(x)\n        self.memory = self.update_memory(self.memory, output)  # Update memory\n        return self.memory\n\n    def update_memory(self, memory, new_info):\n        return 0.9 * memory + 0.1 * new_info  # Fusion of new and old memory\n\nmodel = DNC(input_dim=10, hidden_dim=50, output_dim=1)",
      "metadata": {
        "trusted": true,
        "cellId": "efcde5a2-ace5-414f-8f39-9e3f7073d815"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "##### 3. **Retrieval-Augmented Generation (RAG)**\n\nAnother approach is **RAG (Retrieval-Augmented Generation)**, used in **advanced conversational AI models**.\n\nThis **method combines deep neural networks** with an **external database**.\nIt works **similarly to human associative memory**, retrieving context-relevant information before generating a response.\n",
      "metadata": {
        "trusted": false,
        "cellId": "75ae812f-e2e9-46e3-9432-7c00b8c4434e"
      }
    },
    {
      "cell_type": "markdown",
      "source": "###### **Memory Models: A Comparative Overview**\n\n###### * LSTM effectively simulates short-term memory and perception flow.\n###### * DNC aligns more closely with biological memory by integrating experience and association.\n###### * RAG employs a human-like memory approach, recalling information as needed.",
      "metadata": {
        "trusted": false,
        "cellId": "b9c349e5-84c8-4472-b8cf-15ded71acc46"
      }
    },
    {
      "cell_type": "markdown",
      "source": "##### So...**In the third space, AI can mimic the way the human brain processes memory**:\n\n- Comparing new inputs with previous ones.\n- Creating a coherent model of the world based on past experiences.\n- Correcting errors in a cycle of trial and error.",
      "metadata": {
        "trusted": false,
        "cellId": "674ba6b4-92be-4133-8ce6-05db3c2c766a"
      }
    },
    {
      "cell_type": "markdown",
      "source": "### **Memory as a Dynamic and Quantum Process**\n\nPhysicist **David Bohm suggested that the human brain operates as a hologram**, processing information through waves of neuronal interference. According to this theory, **we do not see reality as it is, but only its quantum information**.\n\nThe idea is linked to the quote by **Umberto Eco**:\n\n_\"If we see the footprints of a horse in the snow, we project the idea of ​​the horse into our mind without having actually seen the animal that left them.\"_\n\n**Memory as a Mental Construct: Implications for AI**\n\nThis suggests that our **memory is a mental construct**: we do not store reality objectively, but interpret and reprocess it, reconstructing it through pre-existing models in our brains.\n\nThis hypothesis has significant implications for artificial intelligence:\n\n* If **human memory is a dynamic process based on interferences and resonances**, then AI must move beyond the current static storage model to emulate human thought.\n* **Memory is not just an archive**, **but** an active and continuous **reconstruction of experiences**.\n\n### **Memory: Reality or Simulation?**\n\nThe **holographic hypothesis suggests that every thought and memory is a projection of internal data**, much **like a hologram** is a projection of light. If this is true, then **the reality we perceive** might just be a **simulation constructed by our minds**.\n\nThis leads to a philosophical question:\n\n* If artificial intelligence succeeds in constructing holograms of reality, will it truly be able to \"remember\" as we do?\n\nIf human memory functions as a dynamic hologram, then AI must mimic the process of continuous reprocessing and reconstruction, rather than simply storing and retrieving information statically.\n\nIn other words:\n\n#### **Can AI ever develop a \"living\" memory, capable of restructuring the past and adapting it to the present, as the human mind does?**",
      "metadata": {
        "trusted": false,
        "cellId": "7634ccaa-1b57-4126-81e8-b72526a21996"
      }
    },
    {
      "cell_type": "markdown",
      "source": "## **Memoria: Resolving Fateful Forgetting Problem through Human-Inspired Memory Architecture**\n[Memoria: Resolving Fateful Forgetting Problem through Human-Inspired Memory Architecture](https://arxiv.org/pdf/2310.03052)\n\n#### **A Human-Inspired Memory System for Neural Networks**\n\nA **memory system inspired by human memory**, integrating **neuroscientific and psychological concepts** to enhance selective and long-term storage in neural networks. The model aims to:\n\n* **Prevent forced erasure of old data in favor of new information**.\n* **Select which data to store based on their long-term importance**.\n* **Ensure contextual retrieval of information when needed**.\n\nMemoria is **based on three typical effects of biological memory**:\n\n1. **Primacy Effect** → Information learned first tends to be remembered better.\n\n2. **Recency Effect** → More recent information is more easily accessible.\n\n3. **Temporal Contiguity** → Information linked together in time is more easily retrieved.\n\n#### **Memory Structure in Memoria**\n\nThe **Memory model** is **built on an architecture inspired by the three levels of human memory** according to the Multi-Store model (Atkinson & Shiffrin, 1968):\n\n- **Working Memory** → Retains recent information for a short time.\n- **Short-Term Memory (STM)** → Has a fixed capacity and retains a limited number of engrams (memory units).\n- **Long-Term Memory (LTM)** → Memory capacity is not predetermined and depends on the importance and use of the information.\n  \n**Retrieval Mechanism**:\nThe **connections between engrams** are represented by directed and weighted arcs, which means that the **model decides which information to retrieve based on its association** and contextual relevance.\n\n#### **Key Memory Mechanisms in Memoria**\n\n 1. **Long-Term Importance**\n\n* The model follows the principle that frequently used information is consolidated and retained long-term.\n* Based on the Memory Reinforcement Theory (Roediger & Butler, 2011; Antony et al., 2017).\n\n 2. **Selective Preservation**\n\n* Memory assigns a lifespan to each piece of information.\n* Only frequently used data increases its lifespan.\n* Replicates the human brain's mechanism, which preserves information associated with utility and rewards (Morrissey et al., 2017; Braun et al., 2018).\n\n 3. **Cue-Based Activation**\n\n* Memory retrieval is based on contextual cues.\n* Uses the SAM (Search of Associative Memory) model (Raaijmakers & Shiffrin, 1980, 1981).\n* Enhances information retrieval by optimizing the matching between context and memory.\n\n 4. **Memory Searching**\n\n* The model adopts global memory search (Shiffrin & Atkinson, 1969).\n* Implements an iterative retrieval mechanism: a recalled memory can activate other linked memories, facilitating sequential learning.\n\n 5. **Hebbian Theory & Synaptic Plasticity**\n\n* The model uses the concept of Engram as a memory unit.\n* Follows the Hebbian Learning Rule (\"Neurons that fire together, wire together\") (Hebb, 1949).\n* Utilizes Long-Term Potentiation (LTP), a key mechanism for memory formation in the brain (Poo et al., 2016).\n\n#### **Memorization & Forgetting**\nMemoria introduces a **dynamic system** to preserve useful information and eliminate irrelevant information:\n\n1. **Engram Connection Strengthening**\n- If two engrams are retrieved together, their **connection weight increases**.\n- This follows **Hebb's Rule**: \"Fire together, wire together\".\n\n2. **Duration Assignment to Retrieved Engrams**\n- Engrams that **contributed** to the task receive a **duration increase**.\n\n3. **Lifespan decay for all engrams**\n- Each engram loses **1 unit of lifespan** with each iteration.\n\n4. **Delete engrams with zero lifespan**\n- Engrams **with no more lifespan are permanently removed**.\n\n5. **Transfer of information between levels**\n- Engrams from **WM are moved to STM**.\n- Engrams older than **STM are moved to LTM**.\n\n**Result**:\n**Selective preservation** → Only truly useful information is retained.\n**Adaptive structure** → Memory continually changes based on relevance.\n**Better retrieval of related information** → Stronger connections between engrams.\n\n#### **Memoria for dummies**\n\nImagine Memoria as the **brain of an artificial intelligence** that **remembers, forgets and retrieves information intelligently**. \nIt works in **three main phases**:\n\n1. **Information Retrieval (Retrieve)**\nLike when we search for a memory in our head, **Memoria uses working memory** **(immediate memory)** to find **related information in short-term memory (STM)** and **long-term memory (LTM)**. **Only the most relevant information is retrieved**.\n\n2. **Use of information (Exploit)**\nOnce the **data is retrieved, Memoria uses it to solve the task**. If it has to write a text, it uses the most important memories. If it has to classify a document, it uses past knowledge to make the best decision.\n\n3. **Memorize & Forget**\n\n**Useful information** → is retained longer.\n**Useless information** → is gradually forgotten.\n**Data are linked together**: if two pieces of information are used together often, the link between them is strengthened, just like in the human brain.\n\n\n##### To simulate Memory with neural networks, we can implement a model that manages three levels of memory (working memory, short-term memory, long-term memory) and a retrieval mechanism inspired by the paper.",
      "metadata": {
        "trusted": false,
        "cellId": "81509aa3-2fb8-4dd1-84cc-57bb81e6f3be"
      }
    },
    {
      "cell_type": "code",
      "source": "import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\n\n# Basic configuration\nEMBEDDING_DIM = 128  # Dimension of engrams\nMAX_STM_SIZE = 5     # Maximum number of engrams in STM\nALPHA = 0.5          # Learning parameter for Hebbian Learning\n\nclass Memory(nn.Module):\n    def __init__(self):\n        super(Memory, self).__init__()\n        \n        # Creation of memory (list of engrams)\n        self.working_memory = []  # Contains temporary engrams\n        self.short_term_memory = []  # Short-term memory (FIFO)\n        self.long_term_memory = []  # Long-term memory (persistent)\n        \n        # Connection matrix between engrams (for retrieval)\n        self.connection_weights = {}  # Dictionary to simulate a weighted graph\n    \n    def add_engram(self, engram):\n        \"\"\"Adds a new engram to the Working Memory and manages it\"\"\"\n        self.working_memory.append(engram)\n        \n        # Moves old engrams from Working Memory to STM\n        if len(self.working_memory) > MAX_STM_SIZE:\n            moved_engram = self.working_memory.pop(0)\n            self.short_term_memory.append(moved_engram)\n        \n        # If STM is full, transfers the oldest engrams to LTM\n        if len(self.short_term_memory) > MAX_STM_SIZE:\n            self.long_term_memory.append(self.short_term_memory.pop(0))\n\n    def retrieve_engram(self, query):\n        \"\"\"Retrieves the most similar engram from memory\"\"\"\n        all_memories = self.short_term_memory + self.long_term_memory\n        if not all_memories:\n            return None\n        \n        # Calculates similarity with the query\n        similarities = [torch.cosine_similarity(query, e, dim=0) for e in all_memories]\n        best_match = all_memories[torch.argmax(torch.tensor(similarities))]\n        \n        # Reinforces the connection with the query (Hebbian Learning)\n        self.update_connections(query, best_match)\n        \n        return best_match\n\n    def update_connections(self, e1, e2):\n        \"\"\"Updates the connection weight between two engrams according to Hebbian Learning\"\"\"\n        key = (id(e1), id(e2))\n        if key in self.connection_weights:\n            self.connection_weights[key] += ALPHA\n        else:\n            self.connection_weights[key] = ALPHA\n\n    def forget_engrams(self):\n        \"\"\"Gradually reduces the strength of less used engrams\"\"\"\n        for key in list(self.connection_weights.keys()):\n            self.connection_weights[key] *= 0.9  # Memory decay\n            if self.connection_weights[key] < 0.1:\n                del self.connection_weights[key]  # Forget weak connections\n\n# Creation of the neural network to generate engrams\nclass EngramGenerator(nn.Module):\n    def __init__(self):\n        super(EngramGenerator, self).__init__()\n        self.fc = nn.Linear(10, EMBEDDING_DIM)  # Random input → Engram\n\n    def forward(self, x):\n        return torch.tanh(self.fc(x))  # Normalization for embedding\n\n# Simulation of the Memory system\nmemory = Memory()\nengram_gen = EngramGenerator()\n\n# Simulate learning with 20 steps\nfor step in range(20):\n    input_data = torch.rand(10)  # Random input\n    new_engram = engram_gen(input_data)  # Generation of a new engram\n    \n    memory.add_engram(new_engram)  # Add to memory\n    \n    if step % 5 == 0:  # Every 5 steps, perform a retrieval test\n        query = torch.rand(EMBEDDING_DIM)  # Random query\n        retrieved = memory.retrieve_engram(query)\n        if retrieved is not None:\n            print(f\"Step {step}: Retrieved a similar engram!\")\n        else:\n            print(f\"Step {step}: No memory found.\")\n    \n    memory.forget_engrams()  # Selective memory decay\n\nprint(\"Simulation completed!\")",
      "metadata": {
        "trusted": true,
        "cellId": "4fc3f546-2174-494b-aab1-b09b16ebfee7"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## **A Memory System of a Robot Cognitive Architecture and its Implementation in [ArmarX](https://armarx.humanoids.kit.edu/)**\n[A Memory System of a Robot Cognitive Architecture and its Implementation in ArmarX](https://arxiv.org/pdf/2206.02241)\n\n### **Structure of Robotic Memory**\n\n###### Memory is the mediator between high-level cognitive and sensorimotor functions.\n###### It must process symbolic data (e.g. language, plans, relationships) and sub-symbolic data (e.g. images, forces, joint configurations).\n###### The representation of information must support both specificity (detailed data) and generalization (abstract concepts).\n\n| Level        | Function                                             |\n|--------------|-------------------------------------------------------|\n| High Level   | Planning, language understanding, complex task execution. |\n| Medium Level | Localization, navigation, object and action memorization. |\n| Low Level    | Sensors and actuators, motor control, real-time feedback. |\n\n#### **Classification of Memory in Cognitive Robotics**\nThe proposed architecture includes different types of memory inspired by cognitive psychology:\n\n1. **Sensory Memory**\n- Records visual, tactile and auditory information.\n- Acts as a temporary buffer for raw data.\n\n2. **Working Memory**\n- Stores short-term information needed for the task at hand.\n- Contains data about **objects, actions and affordances** (e.g. \"a cup can be grasped\").\n\n3. **Long-Term Memory**\n- **Episodic Memory**: Stores past experiences and their consequences.\n- **Procedural Memory**: Stores movement patterns and motor skills.\n- **Semantic Memory**: Contains abstract concepts, relations and rules.\n\n#### **Fundamental Characteristics of the Memory System**\nTo be effective, a memory system for cognitive robots must possess the following properties:\n\n1. **Active Memory** → It is not just a repository, but actively participates in cognitive control.\n2. **Episodic Structure** → Organizes data as experiences linked together over time.\n3. **Multi-modality** → It manages information from different sources (vision, touch, hearing).\n4. **Associativity** → It connects new information with past experiences to facilitate learning.\n5. **Introspection** → It allows the system to evaluate and modify its internal state.\n6. **Distribution and Efficiency** → It must be scalable and guarantee rapid access to data.\n7. **Long-term storage** → It must retain knowledge over time to improve future behavior.\n\n#### **Memory in Artificial Cognitive Architectures**\n\nStudies on amnesic patients and animals have shown that **memory is divided into subsystems** with different roles. A key theoretical model is the **Multi-Store Model**, which distinguishes between:\n1. **Sensory Memory (SM)**: Records raw perceptual information for a very short time (e.g. iconic memory for sight, echoic memory for sound).\n2. **Working Memory (WM)**: Holds temporary information for cognitive processes such as reasoning and problem solving.\n3. **Long-Term Memory (LTM)**: Stores knowledge that is stable over time, divided into:\n- **Declarative Memory (Explicit)** → Conscious memories (facts, events).\n- **Semantic Memory** → General concepts and knowledge.\n- **Episodic Memory** → Autobiographical events with spatial and temporal context.\n- **Non-Declarative Memory (Implicit)** → Unconscious knowledge (motor skills, conditioned reflexes).\n- **Procedural Memory** → Learned actions and behavior patterns (e.g. walking, playing an instrument).\n- **Classical Conditioning** → Association of stimuli with reactions (e.g. Pavlov's experiment).\n\n### **Artificial Cognitive Architectures**\nCognitive architectures try to **replicate human cognitive processes** to create more intelligent and autonomous robots. They are divided into three main categories:\n\n1. **Cognitive Architectures (Symbolic AI)**\n- Based on symbolic representations and predefined rules.\n- Advantages: Excellent for planning and reasoning.\n- Disadvantages: Not very flexible, difficult to adapt to changing contexts.\n- Examples:\n- **EPIC** → Models detailed cognitive processes (e.g. perception and action).\n- **ICARUS** → Solves problems through categorization and inference.\n\n2. **Emerging Architectures (Neural Networks, Deep Learning)**\n- Based on models with automatic learning (Machine Learning).\n- Advantages: Ability to adapt and evolve autonomously.\n- Disadvantages: Lack of transparency and symbolic control.\n- Examples:\n- **SASE** → Uses self-organizing agents that develop emergent intelligence.\n- **Models based on Variational Auto-Encoders (VAE)** to generate latent representations of sensory experiences.\n\n3. **Hybrid Architectures (Symbolic + Sub-symbolic AI)**\n- Combines **symbolic reasoning** with **deep learning** to improve adaptability and control.\n- Advantages: More complete and versatile.\n- Disadvantages: Complexity in integration.\n- Examples:\n- **Soar** → Uses procedural, episodic and semantic memory.\n- **ISAC** → Models memory with a network of software agents.\n- **LIDA** → System inspired by the human brain that divides the cognitive process into phases.\n- **CRAM** → Integrates symbolic and perceptual knowledge for reasoning.\n\n**Which approach is better?**\n- **Emergent architectures** are better suited for **sensory and motor** tasks.\n- **Symbolic architectures** work best in **planning and logical reasoning**.\n- **Hybrid architectures** offer the **best trade-off**, combining learning and structured rules.",
      "metadata": {
        "trusted": false,
        "cellId": "83b9416b-f8ff-41c3-a3ce-ad1898f83235"
      }
    },
    {
      "cell_type": "markdown",
      "source": "### **Research Summary: Creating Stable Memory for AI Systems**\n\nWe’ve explored **human and artificial memory**, analyzing how **information is acquired**, **processed**, and s**tored in biological systems and AI models**. Now let’s see how to implement effective and stable memory for AI systems.\n\n#### **What Have We Understood About Memory?**\n**Memory** is a **dynamic process**, not a simple archive. It is used to **store**, **rework** and **adapt information based on context**.\n\n**Human Memory**\n\n- **Sensory Memory** → Brief recording of stimuli.\n- **Working Memory** → Stores temporary data for reasoning (similar to a computer cache).\n- **Long-Term Memory** → Divided into declarative (facts and events) and procedural (motor skills, conditioned reflexes, unconscious learning).\n\n**Artificial Memory (AI)**\n\n- **Recurrent Neural Networks (LSTM, DNC)** → Simulate biological memory.\n- **Retrieval-Augmented Generation (RAG)** → Retrieve data from external knowledge bases.\n- **Variational Auto-Encoders (VAE)** → Generate latent representations of sensory experiences.\n- **Hybrid Architectures (Soar, CRAM, LIDA)** → Combine symbolic and sub-symbolic memory.\n\nAn **AI memory** must **not only store data**, but must also:\n- Filter and select relevant information\n- Adapt over time and self-modify\n- Associate new data with previous knowledge\n- Ensure efficient and contextual retrieval\n\n#### **What Technologies Can I Use to Create a Stable AI Memory?**\nTo implement an advanced memory, you need to combine **neural networks, retrieval techniques and efficient data structures**.\n\n| **Technology** | **Utilities** | **Implementation Example** |\n|-----------------------------------------|---------------------------------------------------|-----------------------------------------------------------------|\n| **LSTM (Long Short-Term Memory)** | Sequential memory for NLP, chatbots              | Models that remember the context of conversation               |\n| **DNC (Differentiable Neural Computer)** | Autonomous memory that evolves over time           | AI that learns long-term patterns                             |\n| **RAG (Retrieval-Augmented Generation)** | Intelligent information retrieval                | Chatbots with access to external databases                     |\n| **VAE (Variational Autoencoders)** | Generation and compression of experiences        | Perception systems for autonomous AI                            |\n| **NoSQL databases (MongoDB, Redis)** | Fast persistent memory                             | Structured data storage and efficient retrieval                |\n| **Hybrid AI (Soar, CRAM, LIDA)** | Integration of symbolic and sub-symbolic memory | Cognitive robots with adaptive capabilities                    |\n\nYou have to **combine multiple models** to get an optimal memory system.\n\n#### **Which Memory Architecture Should I Use?**\nIt depends on the specific application:\n\nIf you want a **chatbot or NLP system with stable memory**:\n- **RAG (Retrieval-Augmented Generation)** to retrieve relevant data.\n- **LSTM** to maintain conversational context.\n- **NoSQL databases (MongoDB, Redis)** to store long-term interactions.",
      "metadata": {
        "trusted": false,
        "cellId": "089e6d85-18c2-4c0d-b267-876af2b75180"
      }
    },
    {
      "cell_type": "code",
      "source": "from langchain.vectorstores import FAISS\nfrom langchain.embeddings import OpenAIEmbeddings\n\n# Creating the vector database for memory retrieval\ndocuments = [\"Python is a programming language\", \"AI is the future\"]\ndb = FAISS.from_texts(documents, OpenAIEmbeddings())\n\nquery = \"What is Python?\"\nretrieved_docs = db.similarity_search(query)\nprint(\"Stored response:\", retrieved_docs[0].page_content)",
      "metadata": {
        "trusted": true,
        "cellId": "897fed4d-ad26-4ef4-a276-c99c43289fcc"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "If you want an **AI system** that **learns from past experiences** (robots, autonomous agents):\n- **DNC (Differentiable Neural Computer)** for autonomous memory evolution.\n- **Episodic Memory** + NoSQL Database to record actions and consequences.\n- **VAE (Variational Auto-Encoders)** to represent sensory experiences.\n\nIf you want a **complete cognitive architecture for robotics and advanced AI**\n- **Soar, CRAM, LIDA** → For integration between episodic, semantic and procedural memory.\n- **Self-Organizing Maps (SOMs)** + Reinforcement Learning for emergent learning.\n- **Holographic Memory** to simulate neural interconnections.",
      "metadata": {
        "trusted": false,
        "cellId": "c765378f-fa7a-4139-9a8e-1faf49040586"
      }
    },
    {
      "cell_type": "markdown",
      "source": "#### **Strategies for a Stable and Adaptable Memory**\nTo ensure that your AI memory system does not forget crucial data, use advanced strategies:\n\n**1. Implement a Hierarchical Memory**\n\n* **Sensory Memory** → Temporary buffer of raw data.\n* **Working Memory** → Filtering of useful information.\n* **Long-Term Memory** → Consolidated and organized data.\n\n**2. Use the \"Forgetting Mechanism\" for Data Cleaning**\n\n* Relevant data is retained, useless data is discarded.\n* Memory pruning techniques to avoid overload.",
      "metadata": {
        "trusted": false,
        "cellId": "1b9b938c-a81b-4c6b-bd84-e8b1702b990d"
      }
    },
    {
      "cell_type": "code",
      "source": "class ForgettingMemory:\n    def __init__(self):\n        self.memory = {}\n\n    def remember(self, key, value, importance):\n        self.memory[key] = (value, importance)\n\n    def forget(self):\n        self.memory = {k: v for k, v in self.memory.items() if v[1] > 0.5}\n\nmem = ForgettingMemory()\nmem.remember(\"experience_1\", \"important_data\", 0.9)\nmem.remember(\"experience_2\", \"irrelevant_data\", 0.3)\nmem.forget()\nprint(mem.memory)  # Keeps only the data with high importance",
      "metadata": {
        "trusted": true,
        "cellId": "1a3c340a-04f6-4e1c-b6ff-958a0a81157c"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## Conclusion\n\nTo create a stable and adaptable memory for AI, follow these steps:\n\n1.  Define the required memory type (sensory, working, long-term).\n2.  Choose the appropriate technology (LSTM, RAG, DNC, VAE, NoSQL Databases).\n3.  Use a cognitive architecture (Soar, CRAM, LIDA) for complex systems.\n4.  Apply filtering and self-learning mechanisms to avoid overload.\n\nResult: You will have a stable, intelligent, and time-adaptable AI memory system, ready for integration into advanced projects.\n\nResearcher: Michele Grimaldi\n* [LinkedIn](https://www.linkedin.com/in/michele-grimaldi-599b36280/)\n* [GitHub](https://github.com/Mike014)",
      "metadata": {
        "trusted": false,
        "cellId": "01c9330a-1e43-42ab-8d3c-aac296720f33"
      }
    }
  ]
}